{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411b4e2a",
   "metadata": {},
   "source": [
    "# Descargar todos los días"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638edab",
   "metadata": {},
   "source": [
    "## Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32bafebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.net import Fido, attrs as a\n",
    "from sunpy.timeseries import TimeSeries\n",
    "from sunpy.timeseries.sources.goes import XRSTimeSeries\n",
    "import astropy.units as u\n",
    "from sunkit_instruments.goes_xrs import calculate_temperature_em\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sunpy.data import manager\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "from matplotlib import colormaps\n",
    "list(colormaps)\n",
    "#from colorspacious import cspace_converter\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import LogFormatter\n",
    "from matplotlib.ticker import LogFormatterMathtext\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import re\n",
    "import glob\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabecafa",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c68554",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##    1. Download GOES data   #\n",
    "##############################\n",
    "\n",
    "## 1.0. Ensure 1 min resolution (C1)\n",
    "def ensure_1min_resolution(ts):\n",
    "    \"\"\"\n",
    "    Revisa si el TimeSeries está en resolución de 1 minuto.\n",
    "    Si no, lo re-muestrea a 1 min con la media.\n",
    "\n",
    "    Check if the TimeSeries has a resolution of 1 minute.\n",
    "    If no, it will be shown again in 1 minute with the media\n",
    "    \"\"\"\n",
    "    # Pasar a DataFrame\n",
    "    df = ts.to_dataframe()\n",
    "    \n",
    "    # Calcular la resolución actual (diferencia entre los 2 primeros tiempos)\n",
    "    current_res = (df.index[1] - df.index[0]).total_seconds()\n",
    "    \n",
    "    if abs(current_res - 60) < 1:  # ya es 1 min (tolerancia de 1s)\n",
    "        print(\"Resolution = 1 minute\")\n",
    "        return ts\n",
    "    else:\n",
    "        print(f\"Resolution detected: {current_res:.2f} s → resampling at 1 min\")\n",
    "        df_resampled = df.resample(\"1min\").mean()\n",
    "        return TimeSeries(df_resampled, ts.meta)\n",
    "\n",
    "## 1.1. Download data (C1)\n",
    "def Download_Data(start_time, end_time, resolution=\"avg1m\", log_file=None, output_dir=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Descarga datos GOES entre start_time y end_time, asegura resolución 1min,\n",
    "    guarda gráficas en block_dir/data_graphs y log de errores en block_dir.\n",
    "\n",
    "    Entrada / Input:\n",
    "        start_time (str): Tiempo inicial del intervalo de búsqueda, en formato compatible con SunPy.\n",
    "        end_time (str): Tiempo final del intervalo de búsqueda, en formato compatible con SunPy.\n",
    "        resolution (str, opcional / optional): Resolución temporal de los datos GOES. \n",
    "            Opciones válidas: \"flx1s\", \"avg1m\". Por defecto es 'avg1m'.\n",
    "\n",
    "    Salida / Output:\n",
    "        TimeSeries: Objeto de SunPy que contiene los datos XRS del satélite GOES \n",
    "                    dentro del intervalo de tiempo especificado.\n",
    "                    con la resolución especificada.\n",
    "\n",
    "    Descripción / Description:\n",
    "        Esta función busca, descarga y carga datos del instrumento GOES (X-Ray Sensor, XRS)\n",
    "        en el intervalo de tiempo especificado y con la resolución deseada. Utiliza Fido para la búsqueda\n",
    "        y retorna un objeto TimeSeries con los datos.\n",
    "\n",
    "        This function searches for, downloads, and loads data from the GOES (X-Ray Sensor, XRS)\n",
    "        within the specified time interval and chosen resolution. It uses Fido for querying and \n",
    "        returns a TimeSeries object with the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ===== Directorios =====\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            graph_dir = os.path.join(output_dir, \"data_graphs\")\n",
    "            os.makedirs(graph_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"errores_goes.log\")\n",
    "        else:\n",
    "            graph_dir = \"data_graphs\"\n",
    "            os.makedirs(graph_dir, exist_ok=True)\n",
    "            log_file = \"errores_goes.log\"\n",
    "\n",
    "        # ===== Validar resolución =====\n",
    "        valid_resolutions = [\"flx1s\", \"avg1m\"]\n",
    "        if resolution not in valid_resolutions:\n",
    "            raise ValueError(f\"Resolución no válida. Usa una de: {valid_resolutions}\")\n",
    "\n",
    "        # ===== Buscar y descargar datos =====\n",
    "        print(f\"Buscando datos de: {start_time}\")\n",
    "        result = Fido.search(a.Time(start_time, end_time), a.Instrument.goes, a.Resolution(resolution))\n",
    "\n",
    "        if len(result[0]) == 0:\n",
    "            msg = f\"No hay datos GOES para {start_time} - {end_time}. Día saltado.\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None  \n",
    "\n",
    "        print(f\"Descargando datos de {start_time}...\")\n",
    "        files = Fido.fetch(result)\n",
    "\n",
    "        if len(files) == 0:\n",
    "            msg = f\"No se descargaron archivos GOES para {start_time}. Día saltado.\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None\n",
    "\n",
    "        # ===== Cargar datos en TimeSeries =====\n",
    "        try:\n",
    "            ts = TimeSeries(files[0], source=\"XRS\")\n",
    "        except Exception as e:\n",
    "            msg = f\"Error al abrir archivo GOES de {start_time}: {e}\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None\n",
    "\n",
    "        # ===== Asegurar resolución 1 min =====\n",
    "        goes_ts = ensure_1min_resolution(ts)\n",
    "\n",
    "        # ===== Guardar gráfica =====\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        goes_ts.plot(axes=ax)\n",
    "        safe_time = start_time.replace(':','-').replace(' ','_')\n",
    "        output_file = os.path.join(graph_dir, f\"GOES_{safe_time}.png\")\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Gráfica guardada en {output_file}\")\n",
    "\n",
    "        # ===== Extraer observatorio =====\n",
    "        try:\n",
    "            meta0 = goes_ts.meta.metas[0]\n",
    "            platform = meta0.get(\"platform\", \"g16\")\n",
    "            numero = int(\"\".join(filter(str.isdigit, platform)))\n",
    "            observatory = f\"GOES-{numero}\"\n",
    "        except Exception:\n",
    "            observatory = None\n",
    "        print(f\"Observatorio encontrado: {observatory}\")\n",
    "\n",
    "        return goes_ts, observatory\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Error inesperado al descargar datos GOES de {start_time}: {e}\"\n",
    "        print(msg)\n",
    "        if block_dir is not None:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        return None, None  #  siempre devuelve una tupla\n",
    "    \n",
    "\n",
    "## 1.2. Truncar datos\n",
    "def Truncate_Data(goes_ts, flare_start_time, flare_end_time):\n",
    "    \n",
    "    \"\"\"\n",
    "    Entrada / Input:\n",
    "        goes_ts (TimeSeries): Objeto TimeSeries con los datos GOES completos.\n",
    "        flare_start_time (str): Tiempo de inicio de la fulguración (en formato compatible con SunPy).\n",
    "        flare_end_time (str): Tiempo de fin de la fulguración (en formato compatible con SunPy).\n",
    "\n",
    "    Salida / Output:\n",
    "        TimeSeries: Objeto TimeSeries con los datos recortados al intervalo de la fulguración.\n",
    "\n",
    "    Descripción / Description:\n",
    "        Esta función recorta un conjunto de datos GOES a un intervalo de tiempo específico\n",
    "        correspondiente al inicio y fin de una fulguración solar. Si el intervalo no contiene datos,\n",
    "        se lanza una excepción.\n",
    "\n",
    "        This function trims a GOES TimeSeries dataset to a specific time interval\n",
    "        corresponding to the start and end of a solar flare. If the interval contains no data,\n",
    "        an exception is raised.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seleccionar el rango de interés / Select the time range of interest\n",
    "    goes_flare = goes_ts.truncate(flare_start_time, flare_end_time)\n",
    "\n",
    "    # Verificar si hay datos disponibles  / Check if data is available\n",
    "    if len(goes_flare.to_dataframe()) == 0:\n",
    "        raise ValueError(\"El rango de datos seleccionado está vacío. Revisa las fechas.\")\n",
    "        # The selected time range is empty. Please check the input times.\n",
    "\n",
    "    # Visualizar los datos truncados / Plot the trimmed data\n",
    "    goes_flare.peek()\n",
    "\n",
    "    return goes_flare\n",
    "\n",
    "## 2. Background (1)\n",
    "def running_difference(goes_ts, Dif_time=5, plot=False, block_dir=None, start_time=None):\n",
    "    \"\"\"\n",
    "    Calcula las diferencias de flujo de rayos X GOES a un intervalo definido (default 5 min).\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    goes_ts : XRSTimeSeries\n",
    "        Serie temporal original de GOES.\n",
    "    Dif_time : int, optional\n",
    "        Intervalo de diferencia en número de pasos para restar flux (default=5).\n",
    "    plot : bool, optional\n",
    "        Si True, guarda gráficas comparativas original vs corregido.\n",
    "    block_dir : str, optional\n",
    "        Carpeta base donde guardar gráficas.\n",
    "    start_time : str, optional\n",
    "        Tiempo inicial usado para nombrar archivos.\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    goes_diff_ts : XRSTimeSeries\n",
    "        Serie temporal corregida con las diferencias.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.ticker import LogFormatterMathtext\n",
    "\n",
    "    # --- 1. Extraer datos ---\n",
    "    df = goes_ts.to_dataframe()\n",
    "    flux_xrsa = df[\"xrsa\"]\n",
    "    flux_xrsb = df[\"xrsb\"]\n",
    "    npts = len(df)\n",
    "\n",
    "    # --- 2. Calcular diferencias ---\n",
    "    diffa = np.array(flux_xrsa[Dif_time:]) - np.array(flux_xrsa[:npts - Dif_time])\n",
    "    diffb = np.array(flux_xrsb[Dif_time:]) - np.array(flux_xrsb[:npts - Dif_time])\n",
    "\n",
    "    # --- 3. Llenar arreglos completos ---\n",
    "    diffa_full = np.zeros(npts)\n",
    "    diffb_full = np.zeros(npts)\n",
    "    diffa_full[Dif_time:] = diffa\n",
    "    diffb_full[Dif_time:] = diffb\n",
    "\n",
    "    # --- 4. Crear DataFrame corregido ---\n",
    "    df_diff = pd.DataFrame({'xrsa': diffa_full, 'xrsb': diffb_full}, index=df.index)\n",
    "\n",
    "    # --- 5. Crear TimeSeries corregida ---\n",
    "    units = {'xrsa': u.W / u.m**2, 'xrsb': u.W / u.m**2}\n",
    "    goes_diff_ts = XRSTimeSeries(df_diff, units=units, meta=goes_ts.meta)\n",
    "\n",
    "    # --- 6. Función auxiliar para graficar ---\n",
    "    def save_plot(df_orig, df_corr, output_file, title=\"\", logscale=False, positive_only=False):\n",
    "        df_o, df_c = df_orig.copy(), df_corr.copy()\n",
    "        if positive_only:\n",
    "            df_o = df_o.clip(lower=1e-9)\n",
    "            df_c = df_c.clip(lower=1e-9)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        ax.plot(df_o.index, df_o['xrsa'], label='XRSA (original)', color='blue')\n",
    "        ax.plot(df_c.index, df_c['xrsa'], label='XRSA (corrected)', color='blue', linestyle='--')\n",
    "        ax.plot(df_o.index, df_o['xrsb'], label='XRSB (original)', color='red')\n",
    "        ax.plot(df_c.index, df_c['xrsb'], label='XRSB (corrected)', color='red', linestyle='--')\n",
    "\n",
    "        date_only = df_orig.index[0].strftime(\"%Y-%m-%d\")\n",
    "        ax.set_xlabel(f\"Time (UTC) — {date_only}\")\n",
    "        ax.set_ylabel(\"Flux [W/m²]\")\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', ls='--', alpha=0.6)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "        if logscale:\n",
    "            ax.set_yscale('log', base=10)\n",
    "            ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Plot saved at: {output_file}\")\n",
    "\n",
    "    # --- 7. Guardar gráficas si plot=True ---\n",
    "    if plot and block_dir is not None and start_time is not None:\n",
    "        graph_dir = os.path.join(block_dir, \"data_graphs\")\n",
    "        os.makedirs(graph_dir, exist_ok=True)\n",
    "\n",
    "        safe_time = start_time.replace(':','-').replace(' ','_')\n",
    "        df_corr = df_diff\n",
    "\n",
    "        # Linear\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_linear_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison (Δt={Dif_time} steps)\")\n",
    "\n",
    "        # Logarithmic\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_log_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison (Δt={Dif_time} steps) [Log Y]\",\n",
    "                  logscale=True)\n",
    "\n",
    "        # Positive only\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_positive_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison [Positive only, Log Y]\",\n",
    "                  logscale=True,\n",
    "                  positive_only=True)\n",
    "\n",
    "    return goes_diff_ts\n",
    "\n",
    "\n",
    "#####################################\n",
    "# 5. Temperatura y Emission Measure #\n",
    "#####################################\n",
    "\n",
    "## 5.1. Calcular la Temperatura y Medida de emisión con los datos corregidos (C1)\n",
    "#Calculate_Tem_EM(goes_flare_corrected, abundance='photospheric')\n",
    "#Calculate_Tem_EM(goes_flare_corrected, abundance='coronal')\n",
    "def Calculate_Tem_EM(goes_flare_corrected, abundance='coronal'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Entrada / Input:\n",
    "        goes_flare_corrected: Objeto TimeSeries corregido que contiene los datos del flare solar en los\n",
    "        canales XRSA y XRSB. Debe ser compatible con la función `calculate_temperature_em` de SunPy.\n",
    "        abundance (str): Tipo de abundancia elemental a usar en el cálculo (por defecto: 'coronal').\n",
    "                         Otras opciones posibles incluyen 'photospheric', dependiendo del modelo de SunPy.\n",
    "\n",
    "    Salida / Output:\n",
    "        temp_em: Objeto que contiene la temperatura (T) y medida de emisión (EM) derivadas a partir de los\n",
    "        datos GOES corregidos.\n",
    "\n",
    "    Descripción / Description:\n",
    "        Esta función utiliza los datos corregidos del satélite GOES para calcular la temperatura del plasma\n",
    "        y la medida de emisión (EM) durante un evento de fulguración solar. Permite especificar el modelo\n",
    "        de abundancia elemental a utilizar en el cálculo.\n",
    "\n",
    "        This function uses corrected GOES data to compute the plasma temperature and emission measure (EM)\n",
    "        during a solar flare. It allows specifying the elemental abundance model to be used in the calculation.\n",
    "\n",
    "    Notas / Notes:\n",
    "        - Usa la función `calculate_temperature_em` de SunPy.\n",
    "        - El parámetro `abundance` controla el modelo de abundancias (por ejemplo, 'coronal' o 'photospheric').\n",
    "        - Se desactiva temporalmente la verificación del hash de calibración del instrumento.\n",
    "        - Los datos deben estar previamente corregidos y limpios.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Ahora vamos a calcular la T y EM con el modelo de abundancias:{abundance}')\n",
    "    #  Saltar la verificación del hash temporalmente\n",
    "    with manager.skip_hash_check():\n",
    "        #temp_em = calculate_temperature_em(goes_flare_corrected, abundance='coronal')\n",
    "        temp_em = calculate_temperature_em(goes_flare_corrected, abundance)\n",
    "    \n",
    "    print(f'se calculó T y EM con el modelo de abundancias:{abundance}')\n",
    "    #print(temp_em)\n",
    "    return temp_em\n",
    "\n",
    "\n",
    "# mostrar un rango de datos de un TimeSeries\n",
    "def show_range_ts(ts, start_idx, end_idx, height=300):\n",
    "    df = ts.to_dataframe()\n",
    "    subdf = df.iloc[start_idx:end_idx]\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"height:{height}px; overflow:auto; border:1px solid #ccc; padding:10px\">\n",
    "        {subdf.to_html()}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "# Uso:\n",
    "#show_range_ts(goes_ts01, 0, 10)\n",
    "\n",
    "#####################################\n",
    "#     6. Calcula tiempos de FAI     #\n",
    "#####################################\n",
    "\n",
    "def calcular_fai_times(df, \n",
    "                       T_min=7, T_max=14, \n",
    "                       EM_threshold=0.005, \n",
    "                       col_T=\"T_cor\", \n",
    "                       col_EM=\"EM_cor_norm\"):\n",
    "    \"\"\"\n",
    "    Calcula los tiempos en los que se cumplen los criterios del índice FAI.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copiar para no modificar el original\n",
    "    df_fai = df.copy()\n",
    "\n",
    "    # Condiciones del criterio FAI\n",
    "    fai_condition = (\n",
    "        (df_fai[col_T] >= T_min) & \n",
    "        (df_fai[col_T] <= T_max) & \n",
    "        (df_fai[col_EM] > EM_threshold)\n",
    "    )\n",
    "\n",
    "    # Selección\n",
    "    df_fai_selected = df_fai[fai_condition]\n",
    "    fai_times = df_fai_selected.index\n",
    "\n",
    "    # Mostrar los resultados como en tu código original\n",
    "    print(f\"Se encontraron {len(df_fai_selected)} puntos que cumplen el criterio FAI.\\n\")\n",
    "    print(df_fai_selected[[col_T, col_EM]].head())\n",
    "\n",
    "    return fai_times, df_fai_selected\n",
    "\n",
    "########################\n",
    "# 7. Descargar Flares #\n",
    "#######################\n",
    "\n",
    "# Configurar logging básico\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "## 7.1. Descarga de datos de flares\n",
    "def get_flares(start_time, end_time, output_dir=None):\n",
    "    \"\"\"\n",
    "    Busca fulguraciones solares reportadas por GOES en el intervalo dado.\n",
    "    y guarda un log si no se encuentran flares o si hay errores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_time : str\n",
    "        Tiempo inicial (YYYY-MM-DD o compatible con SunPy).\n",
    "    end_time : str\n",
    "        Tiempo final (YYYY-MM-DD o compatible con SunPy).\n",
    "    log_file : str o None, opcional\n",
    "        Ruta del archivo .log donde guardar mensajes si no hay flares o hay errores.\n",
    "        Si es None, no se escribe log en disco (solo en consola).\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame con columnas ['StartTime', 'EndTime', 'Class', 'Observatory', 'PeakTime'] \n",
    "        o None si no se encontraron flares.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Searching for GOES flares between {start_time} and {end_time}...\")\n",
    "        result = Fido.search(a.Time(start_time, end_time), a.hek.FL, a.hek.OBS.Observatory == \"GOES\")\n",
    "\n",
    "        if not result or len(result) == 0 or len(result[0]) == 0:\n",
    "            msg = f\"No solar flares found between {start_time} and {end_time}.\"\n",
    "            logging.info(msg)\n",
    "            if output_dir is not None:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                log_path = os.path.join(output_dir, \"no_flares.log\")\n",
    "                with open(log_path, \"a\") as f:\n",
    "                    f.write(msg + \"\\n\")\n",
    "            return None\n",
    "\n",
    "        # Filtrar columnas 1D\n",
    "        names = [name for name in result[0].colnames if len(result[0][name].shape) <= 1]\n",
    "        table = result[0][names].to_pandas()\n",
    "\n",
    "        # Seleccionar columnas de interés\n",
    "        flare_data = table[[\n",
    "            \"event_starttime\",\n",
    "            \"event_endtime\",\n",
    "            \"fl_goescls\",\n",
    "            \"obs_observatory\",\n",
    "            \"event_peaktime\"\n",
    "        ]]\n",
    "\n",
    "        # Renombrar columnas\n",
    "        flare_data.columns = [\"StartTime\", \"EndTime\", \"Class\", \"Observatory\", \"PeakTime\"]\n",
    "\n",
    "        logging.info(f\"Found {len(flare_data)} GOES solar flares between {start_time} and {end_time}.\")\n",
    "        return flare_data\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"❌ Error retrieving flares between {start_time} and {end_time}: {e}\"\n",
    "        logging.error(msg)\n",
    "\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_path = os.path.join(output_dir, \"no_flares.log\")\n",
    "            with open(log_path, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def build_full_dataframe(goes_ts, goes_corrected, temp_em_cor, temp_em_phot,\n",
    "                         clip_negative=True, normalize_em=False):\n",
    "    \"\"\"\n",
    "    Combina datos originales, corregidos y parámetros de temperatura/EM\n",
    "    en un solo DataFrame.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    goes_ts : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal GOES remuestreada (contiene 'xrsa', 'xrsb').\n",
    "    goes_corrected : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con GOES corregido (xrsa, xrsb corregidos).\n",
    "    temp_em_cor : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con temperatura y EM coronal.\n",
    "    temp_em_phot : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con temperatura y EM fotosférica.\n",
    "    clip_negative : bool, opcional\n",
    "        Si True, reemplaza valores negativos con NaN (en lugar de 0).\n",
    "    normalize_em : bool, opcional\n",
    "        Si True, normaliza EM a unidades de 1e49 cm^-3.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame combinado con todas las columnas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Originales\n",
    "    df_original = goes_ts.to_dataframe()[['xrsa', 'xrsb']]\n",
    "\n",
    "    # Corregidos\n",
    "    df_corr = goes_corrected.to_dataframe().rename(\n",
    "        columns={'xrsa': 'xrsa_corr', 'xrsb': 'xrsb_corr'}\n",
    "    )\n",
    "\n",
    "    # Coronal\n",
    "    df_cor = temp_em_cor.to_dataframe()[['temperature', 'emission_measure']].rename(\n",
    "        columns={'temperature': 'T_cor', 'emission_measure': 'EM_cor'}\n",
    "    )\n",
    "\n",
    "    # Fotosférica\n",
    "    df_phot = temp_em_phot.to_dataframe()[['temperature', 'emission_measure']].rename(\n",
    "        columns={'temperature': 'T_phot', 'emission_measure': 'EM_phot'}\n",
    "    )\n",
    "\n",
    "    # Combinar todo\n",
    "    df_full = pd.concat([df_original, df_corr, df_cor, df_phot], axis=1)\n",
    "\n",
    "    # Opcional: reemplazar valores negativos\n",
    "    if clip_negative:\n",
    "        df_full = df_full.mask(df_full < 0, np.nan)   # ahora quedan NaN y no 0\n",
    "\n",
    "    # Opcional: normalizar EM\n",
    "    if normalize_em:\n",
    "        df_full['EM_cor_norm'] = df_full['EM_cor'] / 1e49\n",
    "        df_full['EM_phot_norm'] = df_full['EM_phot'] / 1e49\n",
    "\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# agrupa por clases de flares en 2 grupos por clase\n",
    "def assign_flare_group(flare_class):\n",
    "    \"\"\"\n",
    "    Asigna un grupo a un flare según su clase y subnivel.\n",
    "    \n",
    "    Ejemplos:\n",
    "    - C3.0 → C1-4\n",
    "    - C7.5 → C5-9\n",
    "    - X2.0 → X1-4\n",
    "    - X7.0 → X5+\n",
    "    \"\"\"\n",
    "    group_ranges = {\n",
    "        \"A\": [(1, 4), (5, 9)],\n",
    "        \"B\": [(1, 4), (5, 9)],\n",
    "        \"C\": [(1, 4), (5, 9)],\n",
    "        \"M\": [(1, 4), (5, 9)],\n",
    "        \"X\": [(1, 4), (5, 1000)]  # 1000 actúa como \"infinito\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        letter = flare_class[0]  # A, B, C, M, X\n",
    "        number = float(flare_class[1:])  # acepta decimales, ej: \"2.7\" → 2.7\n",
    "    except:\n",
    "        return flare_class  # si algo raro viene en el CSV\n",
    "\n",
    "    if letter not in group_ranges:\n",
    "        return flare_class\n",
    "\n",
    "    for (low, high) in group_ranges[letter]:\n",
    "        if low <= number <= high:\n",
    "            if high >= 1000:  # caso abierto (ej: X5+)\n",
    "                return f\"{letter}{low}+\"\n",
    "            return f\"{letter}{low}-{high}\"\n",
    "    \n",
    "    return flare_class\n",
    "\n",
    "\n",
    "#######################\n",
    "### cuenta los días ###\n",
    "#######################\n",
    "\n",
    "def count_days(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Counts the number of days between two dates.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of days between start_date and end_date\n",
    "    \"\"\"\n",
    "    # Convert the strings to datetime objects\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = end - start\n",
    "    \n",
    "    return difference.days\n",
    "\n",
    "# Example usage\n",
    "#days = count_days(\"1980-01-05\", \"2025-08-20\")\n",
    "#print(f\"Days between the dates: {days}\")\n",
    "\n",
    "# Dataframe con todas las fechas\n",
    "def all_dates_dataframe(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con todas las fechas entre start_date y end_date (inclusive).\n",
    "\n",
    "    Parámetros:\n",
    "    - start_date (str): Fecha inicial en formato 'YYYY-MM-DD'\n",
    "    - end_date (str): Fecha final en formato 'YYYY-MM-DD'\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame con columnas ['start_time', 'end_time']\n",
    "    \"\"\"\n",
    "    # Convertir a objetos datetime\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Generar todas las fechas día a día\n",
    "    all_days = [start + timedelta(days=i) for i in range((end - start).days + 1)]\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    data = {\n",
    "        \"start_time\": [d.strftime(\"%Y-%m-%d 00:00:00\") for d in all_days],\n",
    "        \"end_time\":   [d.strftime(\"%Y-%m-%d 23:59:00\") for d in all_days]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#############################################\n",
    "# elige los días cada cierto paso según     #\n",
    "# el número de días que quiera analizar (n) #\n",
    "#############################################\n",
    "\n",
    "def select_dates(start_date, end_date, n=10):\n",
    "    \"\"\"\n",
    "    Selects n dates evenly spaced between start_date and end_date.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format\n",
    "    - n (int): Number of dates to select (default 10)\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of dates in 'YYYY-MM-DD' format\n",
    "    \"\"\"\n",
    "    # Convert strings to datetime objects\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate total number of days\n",
    "    total_days = (end - start).days\n",
    "    \n",
    "    # Calculate integer step\n",
    "    #step = total_days // (n - 1)  # n-1 intervals for n dates\n",
    "    \n",
    "    if n < 2:\n",
    "        return [start_date]\n",
    "\n",
    "    # step aproximado al entero más cercano\n",
    "    step = max(1, int(round(total_days / (n - 1))))\n",
    "    print(step)\n",
    "    # Generate the dates\n",
    "    dates = [start + timedelta(days=i*step) for i in range(n)]\n",
    "    print(len(dates))\n",
    "    # Convert to strings\n",
    "    return [d.strftime(\"%Y-%m-%d\") for d in dates]\n",
    "\n",
    "\n",
    "# guarda los datos seleccionados segun el paso en un df\n",
    "def dates_to_dataframe(dates_list):\n",
    "    \"\"\"\n",
    "    Converts a list of dates into a DataFrame with start_time and end_time.\n",
    "\n",
    "    Parameters:\n",
    "    - dates_list (list of str): List of dates in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with columns ['start_time', 'end_time']\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"start_time\": [f\"{date} 00:00:00\" for date in dates_list],\n",
    "        \"end_time\":   [f\"{date} 23:59:00\" for date in dates_list]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Divide un dateframen en lotes:\n",
    "def chunk_dataframe(df, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en bloques (chunks) de tamaño chunk_size.\n",
    "    Devuelve una lista de DataFrames.\n",
    "    \"\"\"\n",
    "    return [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# unir todos los bloques al final\n",
    "def combine_blocks(output_dir, pattern, final_name, time_column=None):\n",
    "    \"\"\"\n",
    "    Une todos los CSV de bloques en un único archivo final.\n",
    "    Si los CSV tienen la primera columna como fecha sin nombre, se usa para ordenar.\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(output_dir, \"**\", f\"*{pattern}*.csv\"), recursive=True))\n",
    "    df_list = []\n",
    "\n",
    "    for f in files:\n",
    "        # Leer CSV; si la primera columna no tiene nombre, le ponemos \"time\"\n",
    "        df = pd.read_csv(f)\n",
    "        if df.columns[0] == \"\":\n",
    "            df = pd.read_csv(f, names=[\"time\"] + list(df.columns[1:]), header=0)\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Ordenar según columna de tiempo\n",
    "        if time_column is None:\n",
    "            # usar la primera columna como tiempo\n",
    "            combined.iloc[:, 0] = pd.to_datetime(combined.iloc[:, 0])\n",
    "            combined = combined.sort_values(by=combined.columns[0]).reset_index(drop=True)\n",
    "            print(f\"✅ Ordenado por la primera columna (usada como tiempo)\")\n",
    "        elif time_column in combined.columns:\n",
    "            combined[time_column] = pd.to_datetime(combined[time_column])\n",
    "            combined = combined.sort_values(by=time_column).reset_index(drop=True)\n",
    "            print(f\"✅ Ordenado por columna '{time_column}'\")\n",
    "        else:\n",
    "            print(f\"⚠️ Columna '{time_column}' no encontrada, no se ordenó\")\n",
    "\n",
    "        combined.to_csv(os.path.join(output_dir, final_name), index=False)\n",
    "        print(f\"{final_name} creado con {len(combined)} filas (desde {len(files)} archivos).\")\n",
    "    else:\n",
    "        print(f\"No se encontraron archivos con patrón {pattern}\")\n",
    "\n",
    "# Example usage\n",
    "#dates_list = select_dates(\"1980-01-05\", \"2025-08-20\", n=10)\n",
    "#df_intervals = dates_to_dataframe(dates_list)\n",
    "\n",
    "#print(df_intervals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf67b99",
   "metadata": {},
   "source": [
    "## Función completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bd5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_goes_flare_data(start_time, end_time, \n",
    "                      resolution=\"avg1m\",  #resolución de descarga de los datos GOES (1min)\n",
    "                      Dif_time=5,            #Difference time Δt (for background)\n",
    "                      plot_diff=True,\n",
    "                      output_dir=None):\n",
    "    \"\"\"\n",
    "    Pipeline completo para análisis de datos GOES y cálculo de FAI y anticipación.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    start_time : str\n",
    "        Tiempo inicial (ej: \"2017-09-06 12:00:00\")\n",
    "    end_time : str\n",
    "        Tiempo final (ej: \"2017-09-06 12:15:00\")\n",
    "    resolution : str, opcional\n",
    "        Resolución de datos GOES, default \"avg1m\". Opciones: [\"flx1s\", \"avg1m\"]\n",
    "    Dif_time : int, opcional\n",
    "        Ventana en pasos para calcular diferencias (default 5).\n",
    "    plot_diff : bool, opcional\n",
    "        Si True, grafica las diferencias calculadas.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict con:\n",
    "        - df_full : DataFrame con todos los cálculos\n",
    "        - df_flare_data : DataFrame de flares GOES en el intervalo\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Descargar datos GOES\n",
    "    print(\"1. Descargar datos GOES\")\n",
    "    data = Download_Data(start_time, end_time, resolution, log_file=\"errores_goes.log\", output_dir=output_dir)\n",
    "    # para días sin datos GOES\n",
    "    if data is None:\n",
    "        print(f\"No hay datos GOES para {start_time} - {end_time}. Día saltado.\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Se encontraron datos  GOES para {start_time} - {end_time}. Día saltado.\\n\")\n",
    "        goes_ts, observatory = data\n",
    "        # Convertir TimeSeries a DataFrame para inspección\n",
    "        df_goes = goes_ts.to_dataframe()\n",
    "\n",
    "        # Número de registros\n",
    "        print(f\"Número de registros: {len(df_goes)}\")\n",
    "\n",
    "        # Columnas disponibles\n",
    "        print(f\"Columnas disponibles: {list(df_goes.columns)}\")\n",
    "        \n",
    "    print(\"2. Restar Background\")\n",
    "    # 2. Calcular diferencias\n",
    "    goes_ts_corrected_diff = running_difference(goes_ts, Dif_time=Dif_time, plot=plot_diff, block_dir=output_dir, start_time=start_time)\n",
    "                            \n",
    "\n",
    "    # 3. Calcular T y EM (coronal y fotosférico)\n",
    "    print(\"3. USAR FUNCIÓN SUNPY calculate_t_em\")\n",
    "    temp_em_cor = Calculate_Tem_EM(goes_ts_corrected_diff, abundance='coronal')\n",
    "    temp_em_phot = Calculate_Tem_EM(goes_ts_corrected_diff, abundance='photospheric')\n",
    "\n",
    "    print(\"4. CONSTRUIR df_full\")\n",
    "    # 4. Construir dataframe completo\n",
    "    df_full = build_full_dataframe(goes_ts, goes_ts_corrected_diff, temp_em_cor, temp_em_phot,\n",
    "                         clip_negative=True, normalize_em=True)\n",
    "\n",
    "    print(f\"5. añadiendo observatorio: GOES: {observatory}\")\n",
    "    # 5. Añadir columna del observatorio\n",
    "    df_full[\"observatory\"] = observatory\n",
    "    # Reemplazar NaN por \"Unknown\"\n",
    "    df_full[\"observatory\"] = df_full[\"observatory\"].fillna(\"Unknown\")\n",
    "    # Mover \"observatory\" al inicio\n",
    "    cols = [\"observatory\"] + [col for col in df_full.columns if col != \"observatory\"]\n",
    "    df_full = df_full[cols]\n",
    "    #print(df_full)\n",
    "    print(f\"se añadió observatorio: GOES: {observatory}\")\n",
    "\n",
    "    #print(f\"6. Normalizando EM\")\n",
    "    # 6. Normalizar EM\n",
    "    #df_full['EM_cor_norm'] = df_full['EM_cor'] / 1e49\n",
    "    #df_full['EM_phot_norm'] = df_full['EM_phot'] / 1e49\n",
    "    \n",
    "    print(f\"6. Descargando flares: {start_time} - {end_time}\")\n",
    "    # 6. Descargar flares\n",
    "    flare_data = get_flares(start_time, end_time, output_dir=output_dir)\n",
    "    \n",
    "    #verifica si encontró flares para el día\n",
    "    if flare_data is None:\n",
    "        print(f\"No se encontraron flares para el intervalo {start_time} - {end_time}. Saltando...\")\n",
    "        # Crear DataFrames vacíos para todos los resultados que dependen de flares\n",
    "        df_flare_data = pd.DataFrame()\n",
    "    \n",
    "        \n",
    "        # Retornar diccionario con flares vacíos\n",
    "        return {\n",
    "            \"df_full\": df_full,\n",
    "            \"df_flare_data\": df_flare_data,\n",
    "        }\n",
    "    \n",
    "\n",
    "    # Si flare_data no es None continúa\n",
    "    print(f\"Se descargaron flares para el intervalo {start_time} - {end_time}.\")\n",
    "    df_flare_data = flare_data[\n",
    "        flare_data['Class'].notna() & \n",
    "        (flare_data['Class'].str.strip() != \"\") &\n",
    "        (flare_data['Observatory'] == \"GOES\")\n",
    "    ]\n",
    "    print(f\"7. Se filtraron solo flares de GOES.\")\n",
    "      \n",
    "\n",
    "    # Retornar todo como dict\n",
    "    return {\n",
    "        \"df_full\": df_full,\n",
    "        \"df_flare_data\": df_flare_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fcdb6",
   "metadata": {},
   "source": [
    "## Días para cada bloque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days between the dates: 283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7753424657534247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size=365 # días para cada bloque\n",
    "\n",
    "# contamos cuantos días hay desde la primera fecha 1980-01-05 hasta hoy\n",
    "year = 1980\n",
    "days = count_days(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "print(f\"Days between the dates: {days}\")\n",
    "\n",
    "days_per_block = days/chunk_size\n",
    "days_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e923b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              start_time             end_time\n",
      "0    1980-01-01 00:00:00  1980-01-01 23:59:00\n",
      "1    1980-01-02 00:00:00  1980-01-02 23:59:00\n",
      "2    1980-01-03 00:00:00  1980-01-03 23:59:00\n",
      "3    1980-01-04 00:00:00  1980-01-04 23:59:00\n",
      "4    1980-01-05 00:00:00  1980-01-05 23:59:00\n",
      "..                   ...                  ...\n",
      "279  1980-10-06 00:00:00  1980-10-06 23:59:00\n",
      "280  1980-10-07 00:00:00  1980-10-07 23:59:00\n",
      "281  1980-10-08 00:00:00  1980-10-08 23:59:00\n",
      "282  1980-10-09 00:00:00  1980-10-09 23:59:00\n",
      "283  1980-10-10 00:00:00  1980-10-10 23:59:00\n",
      "\n",
      "[284 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_days = all_dates_dataframe(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "print(df_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbba1b2",
   "metadata": {},
   "source": [
    "## New folder for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbffd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener fecha actual en formato YYYY-MM-DD\n",
    "fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#fecha_actual =\"2025-10-07\"\n",
    "# Crear nombre dinámico de la carpeta\n",
    "output_dir = f\"{fecha_actual}_Analysis_for_{year}\"\n",
    "#output_dir = f\"2025-10-07_Analysis_for_1000_days\"\n",
    "# Crear carpeta (si no existe)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee842",
   "metadata": {},
   "source": [
    "## Procesar por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34a5a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para procesar y guardar datos por mes\n",
    "def process_goes_by_month(df_days, output_dir):\n",
    "    \"\"\"\n",
    "    Procesa los datos GOES agrupando los días por mes.\n",
    "\n",
    "    Crea una carpeta por mes (YYYY_MM) dentro de output_dir,\n",
    "    y guarda los CSV 'df_full' y 'df_flare_data' para cada mes.\n",
    "\n",
    "    Parámetros:\n",
    "    - df_days: DataFrame con columnas ['start_time', 'end_time']\n",
    "    - output_dir: ruta base donde se guardarán los resultados\n",
    "    \"\"\"\n",
    "    # Asegurar que las columnas de fecha son tipo datetime\n",
    "    df_days[\"start_time\"] = pd.to_datetime(df_days[\"start_time\"])\n",
    "    df_days[\"end_time\"]   = pd.to_datetime(df_days[\"end_time\"])\n",
    "\n",
    "    # Crear columna auxiliar con el mes en formato YYYY_MM\n",
    "    df_days[\"month\"] = df_days[\"start_time\"].dt.strftime(\"%Y_%m\")\n",
    "\n",
    "    # Agrupar por mes\n",
    "    for month, df_month in df_days.groupby(\"month\"):\n",
    "        print(f\"\\n🟦 Procesando mes {month} ({len(df_month)} días)\")\n",
    "\n",
    "        # Crear carpeta para este mes\n",
    "        month_dir = os.path.join(output_dir, f\"Month_{month}\")\n",
    "        log_file = os.path.join(month_dir, \"errores_goes.log\")\n",
    "        os.makedirs(month_dir, exist_ok=True)\n",
    "\n",
    "        # Archivos esperados\n",
    "        file_full_month  = os.path.join(month_dir, f\"df_full_{month}.csv\")\n",
    "        file_flare_month = os.path.join(month_dir, f\"df_flare_data_{month}.csv\")\n",
    "\n",
    "        # Saltar si ya existen ambos\n",
    "        if os.path.exists(file_full_month) and os.path.exists(file_flare_month):\n",
    "            print(f\"⚠️ Mes {month} ya procesado, saltando...\")\n",
    "            continue\n",
    "\n",
    "        list_df_full = []\n",
    "        list_df_flare_data = []\n",
    "\n",
    "        # Procesar cada día dentro del mes\n",
    "        for idx, row in df_month.iterrows():\n",
    "            start_time = row[\"start_time\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            end_time   = row[\"end_time\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            date_str   = row[\"start_time\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            print(f\"\\n🚀 Procesando {date_str} ({start_time} → {end_time})...\")\n",
    "\n",
    "            try:\n",
    "                results = download_goes_flare_data(   #función completa\n",
    "                                    start_time, end_time,\n",
    "                                    resolution=\"avg1m\",\n",
    "                                    Dif_time=5,\n",
    "                                    plot_diff=True,\n",
    "                                    output_dir=month_dir\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error en {date_str}: {e}\")\n",
    "                results = None\n",
    "\n",
    "            if results is None:\n",
    "                print(f\"⚠️ Sin resultados para {date_str}\")\n",
    "                continue\n",
    "\n",
    "            df_full = results[\"df_full\"]\n",
    "            df_flare_data = results[\"df_flare_data\"]\n",
    "\n",
    "            if not df_full.empty:\n",
    "                list_df_full.append(df_full)\n",
    "                print(f\"✅ df_full agregado ({date_str})\")\n",
    "            if not df_flare_data.empty:\n",
    "                list_df_flare_data.append(df_flare_data)\n",
    "                print(f\"✅ df_flare_data agregado ({date_str})\")\n",
    "\n",
    "        # Guardar resultados del mes\n",
    "        if list_df_full:\n",
    "            pd.concat(list_df_full).to_csv(file_full_month, index=True)\n",
    "            print(f\"💾 df_full_{month}.csv guardado\")\n",
    "        else:\n",
    "            pd.DataFrame().to_csv(file_full_month, index=False)\n",
    "            print(f\"ℹ️ df_full_{month}.csv vacío\")\n",
    "\n",
    "        if list_df_flare_data:\n",
    "            pd.concat(list_df_flare_data).to_csv(file_flare_month, index=True)\n",
    "            print(f\"💾 df_flare_data_{month}.csv guardado\")\n",
    "        else:\n",
    "            pd.DataFrame().to_csv(file_flare_month, index=False)\n",
    "            print(f\"ℹ️ df_flare_data_{month}.csv vacío\")\n",
    "\n",
    "        print(f\"🏁 Mes {month} terminado → {month_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598c5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟦 Procesando mes 2008_02 (2 días)\n",
      "\n",
      "🚀 Procesando 2008-02-03 (2008-02-03 00:00:00 → 2008-02-03 23:59:00)...\n",
      "1. Descargar datos GOES\n",
      "Buscando datos de: 2008-02-03 00:00:00\n",
      "Descargando datos de 2008-02-03 00:00:00...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.69file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution = 1 minute\n",
      "Gráfica guardada en 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_2008-02-03_00-00-00.png\n",
      "Observatorio encontrado: GOES-11\n",
      "Se encontraron datos  GOES para 2008-02-03 00:00:00 - 2008-02-03 23:59:00. Día saltado.\n",
      "\n",
      "Número de registros: 1440\n",
      "Columnas disponibles: ['xrsa', 'xrsb', 'xrsa_quality', 'xrsb_quality']\n",
      "2. Restar Background\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_linear_2008-02-03_00-00-00.png\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_log_2008-02-03_00-00-00.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1962144/3253930153.py:225: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_o = df_o.clip(lower=1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_positive_2008-02-03_00-00-00.png\n",
      "3. USAR FUNCIÓN SUNPY calculate_t_em\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:coronal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.32file/s]\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: divide by zero encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: invalid value encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calculó T y EM con el modelo de abundancias:coronal\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:photospheric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded:   0%|          | 0/1 [00:00<?, ?file/s]Exception ignored in: <function BaseEventLoop.__del__ at 0x7f3e3a99b420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/base_events.py\", line 766, in __del__\n",
      "    self.close()\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 73, in close\n",
      "    self.remove_signal_handler(sig)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 162, in remove_signal_handler\n",
      "    signal.signal(sig, handler)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "ValueError: signal only works in main thread of the main interpreter\n",
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.28file/s]\n",
      "2025-10-16 17:43:37 - root - INFO: Searching for GOES flares between 2008-02-03 00:00:00 and 2008-02-03 23:59:00...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calculó T y EM con el modelo de abundancias:photospheric\n",
      "4. CONSTRUIR df_full\n",
      "5. añadiendo observatorio: GOES: GOES-11\n",
      "se añadió observatorio: GOES: GOES-11\n",
      "6. Descargando flares: 2008-02-03 00:00:00 - 2008-02-03 23:59:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 17:43:38 - root - INFO: No solar flares found between 2008-02-03 00:00:00 and 2008-02-03 23:59:00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron flares para el intervalo 2008-02-03 00:00:00 - 2008-02-03 23:59:00. Saltando...\n",
      "✅ df_full agregado (2008-02-03)\n",
      "\n",
      "🚀 Procesando 2008-02-04 (2008-02-04 00:00:00 → 2008-02-04 23:59:00)...\n",
      "1. Descargar datos GOES\n",
      "Buscando datos de: 2008-02-04 00:00:00\n",
      "Descargando datos de 2008-02-04 00:00:00...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.75file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution = 1 minute\n",
      "Gráfica guardada en 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_2008-02-04_00-00-00.png\n",
      "Observatorio encontrado: GOES-11\n",
      "Se encontraron datos  GOES para 2008-02-04 00:00:00 - 2008-02-04 23:59:00. Día saltado.\n",
      "\n",
      "Número de registros: 1440\n",
      "Columnas disponibles: ['xrsa', 'xrsb', 'xrsa_quality', 'xrsb_quality']\n",
      "2. Restar Background\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_linear_2008-02-04_00-00-00.png\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_log_2008-02-04_00-00-00.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1962144/3253930153.py:225: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_o = df_o.clip(lower=1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_positive_2008-02-04_00-00-00.png\n",
      "3. USAR FUNCIÓN SUNPY calculate_t_em\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:coronal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.24file/s]\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: divide by zero encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: invalid value encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calculó T y EM con el modelo de abundancias:coronal\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:photospheric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded:   0%|          | 0/1 [00:00<?, ?file/s]Exception ignored in: <function BaseEventLoop.__del__ at 0x7f3e3a99b420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/base_events.py\", line 766, in __del__\n",
      "    self.close()\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 73, in close\n",
      "    self.remove_signal_handler(sig)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 162, in remove_signal_handler\n",
      "    signal.signal(sig, handler)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "ValueError: signal only works in main thread of the main interpreter\n",
      "Files Downloaded: 100%|██████████| 1/1 [00:00<00:00,  1.31file/s]\n",
      "2025-10-16 17:43:51 - root - INFO: Searching for GOES flares between 2008-02-04 00:00:00 and 2008-02-04 23:59:00...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calculó T y EM con el modelo de abundancias:photospheric\n",
      "4. CONSTRUIR df_full\n",
      "5. añadiendo observatorio: GOES: GOES-11\n",
      "se añadió observatorio: GOES: GOES-11\n",
      "6. Descargando flares: 2008-02-04 00:00:00 - 2008-02-04 23:59:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 17:43:52 - root - INFO: No solar flares found between 2008-02-04 00:00:00 and 2008-02-04 23:59:00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron flares para el intervalo 2008-02-04 00:00:00 - 2008-02-04 23:59:00. Saltando...\n",
      "✅ df_full agregado (2008-02-04)\n",
      "💾 df_full_2008_02.csv guardado\n",
      "ℹ️ df_flare_data_2008_02.csv vacío\n",
      "🏁 Mes 2008_02 terminado → 2025-10-16_Analysis_for_1980/Month_2008_02\n"
     ]
    }
   ],
   "source": [
    "#df_days = all_dates_dataframe(\"2008-02-03\", \"2008-02-04\")\n",
    "process_goes_by_month(df_days, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "052ee379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ordenado por la primera columna (usada como tiempo)\n",
      "all_df_full_1980.csv creado con 362758 filas (desde 15 archivos).\n",
      "✅ Ordenado por columna 'StartTime'\n",
      "all_df_flare_data_1980.csv creado con 1585 filas (desde 15 archivos).\n"
     ]
    }
   ],
   "source": [
    "# Unir cada salida\n",
    "combine_blocks(output_dir, \"df_full_block\", f\"all_df_full_{year}.csv\", time_column=None)\n",
    "combine_blocks(output_dir, \"df_flare_data_block\", f\"all_df_flare_data_{year}.csv\", time_column=\"StartTime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf30f6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
