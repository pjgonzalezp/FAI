{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411b4e2a",
   "metadata": {},
   "source": [
    "# Descargar todos los d√≠as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638edab",
   "metadata": {},
   "source": [
    "## Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32bafebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.net import Fido, attrs as a\n",
    "from sunpy.timeseries import TimeSeries\n",
    "from sunpy.timeseries.sources.goes import XRSTimeSeries\n",
    "import astropy.units as u\n",
    "from sunkit_instruments.goes_xrs import calculate_temperature_em\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sunpy.data import manager\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "from matplotlib import colormaps\n",
    "list(colormaps)\n",
    "#from colorspacious import cspace_converter\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import LogFormatter\n",
    "from matplotlib.ticker import LogFormatterMathtext\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import re\n",
    "import glob\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabecafa",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c68554",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##    1. Download GOES data   #\n",
    "##############################\n",
    "\n",
    "## 1.0. Ensure 1 min resolution (C1)\n",
    "def ensure_1min_resolution(ts):\n",
    "    \"\"\"\n",
    "    Revisa si el TimeSeries est√° en resoluci√≥n de 1 minuto.\n",
    "    Si no, lo re-muestrea a 1 min con la media.\n",
    "\n",
    "    Check if the TimeSeries has a resolution of 1 minute.\n",
    "    If no, it will be shown again in 1 minute with the media\n",
    "    \"\"\"\n",
    "    # Pasar a DataFrame\n",
    "    df = ts.to_dataframe()\n",
    "    \n",
    "    # Calcular la resoluci√≥n actual (diferencia entre los 2 primeros tiempos)\n",
    "    current_res = (df.index[1] - df.index[0]).total_seconds()\n",
    "    \n",
    "    if abs(current_res - 60) < 1:  # ya es 1 min (tolerancia de 1s)\n",
    "        print(\"Resolution = 1 minute\")\n",
    "        return ts\n",
    "    else:\n",
    "        print(f\"Resolution detected: {current_res:.2f} s ‚Üí resampling at 1 min\")\n",
    "        df_resampled = df.resample(\"1min\").mean()\n",
    "        return TimeSeries(df_resampled, ts.meta)\n",
    "\n",
    "## 1.1. Download data (C1)\n",
    "def Download_Data(start_time, end_time, resolution=\"avg1m\", log_file=None, output_dir=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Descarga datos GOES entre start_time y end_time, asegura resoluci√≥n 1min,\n",
    "    guarda gr√°ficas en block_dir/data_graphs y log de errores en block_dir.\n",
    "\n",
    "    Entrada / Input:\n",
    "        start_time (str): Tiempo inicial del intervalo de b√∫squeda, en formato compatible con SunPy.\n",
    "        end_time (str): Tiempo final del intervalo de b√∫squeda, en formato compatible con SunPy.\n",
    "        resolution (str, opcional / optional): Resoluci√≥n temporal de los datos GOES. \n",
    "            Opciones v√°lidas: \"flx1s\", \"avg1m\". Por defecto es 'avg1m'.\n",
    "\n",
    "    Salida / Output:\n",
    "        TimeSeries: Objeto de SunPy que contiene los datos XRS del sat√©lite GOES \n",
    "                    dentro del intervalo de tiempo especificado.\n",
    "                    con la resoluci√≥n especificada.\n",
    "\n",
    "    Descripci√≥n / Description:\n",
    "        Esta funci√≥n busca, descarga y carga datos del instrumento GOES (X-Ray Sensor, XRS)\n",
    "        en el intervalo de tiempo especificado y con la resoluci√≥n deseada. Utiliza Fido para la b√∫squeda\n",
    "        y retorna un objeto TimeSeries con los datos.\n",
    "\n",
    "        This function searches for, downloads, and loads data from the GOES (X-Ray Sensor, XRS)\n",
    "        within the specified time interval and chosen resolution. It uses Fido for querying and \n",
    "        returns a TimeSeries object with the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ===== Directorios =====\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            graph_dir = os.path.join(output_dir, \"data_graphs\")\n",
    "            os.makedirs(graph_dir, exist_ok=True)\n",
    "            log_file = os.path.join(output_dir, \"errores_goes.log\")\n",
    "        else:\n",
    "            graph_dir = \"data_graphs\"\n",
    "            os.makedirs(graph_dir, exist_ok=True)\n",
    "            log_file = \"errores_goes.log\"\n",
    "\n",
    "        # ===== Validar resoluci√≥n =====\n",
    "        valid_resolutions = [\"flx1s\", \"avg1m\"]\n",
    "        if resolution not in valid_resolutions:\n",
    "            raise ValueError(f\"Resoluci√≥n no v√°lida. Usa una de: {valid_resolutions}\")\n",
    "\n",
    "        # ===== Buscar y descargar datos =====\n",
    "        print(f\"Buscando datos de: {start_time}\")\n",
    "        result = Fido.search(a.Time(start_time, end_time), a.Instrument.goes, a.Resolution(resolution))\n",
    "\n",
    "        if len(result[0]) == 0:\n",
    "            msg = f\"No hay datos GOES para {start_time} - {end_time}. D√≠a saltado.\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None  \n",
    "\n",
    "        print(f\"Descargando datos de {start_time}...\")\n",
    "        files = Fido.fetch(result)\n",
    "\n",
    "        if len(files) == 0:\n",
    "            msg = f\"No se descargaron archivos GOES para {start_time}. D√≠a saltado.\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None\n",
    "\n",
    "        # ===== Cargar datos en TimeSeries =====\n",
    "        try:\n",
    "            ts = TimeSeries(files[0], source=\"XRS\")\n",
    "        except Exception as e:\n",
    "            msg = f\"Error al abrir archivo GOES de {start_time}: {e}\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return None, None\n",
    "\n",
    "        # ===== Asegurar resoluci√≥n 1 min =====\n",
    "        goes_ts = ensure_1min_resolution(ts)\n",
    "\n",
    "        # ===== Guardar gr√°fica =====\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        goes_ts.plot(axes=ax)\n",
    "        safe_time = start_time.replace(':','-').replace(' ','_')\n",
    "        output_file = os.path.join(graph_dir, f\"GOES_{safe_time}.png\")\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Gr√°fica guardada en {output_file}\")\n",
    "\n",
    "        # ===== Extraer observatorio =====\n",
    "        try:\n",
    "            meta0 = goes_ts.meta.metas[0]\n",
    "            platform = meta0.get(\"platform\", \"g16\")\n",
    "            numero = int(\"\".join(filter(str.isdigit, platform)))\n",
    "            observatory = f\"GOES-{numero}\"\n",
    "        except Exception:\n",
    "            observatory = None\n",
    "        print(f\"Observatorio encontrado: {observatory}\")\n",
    "\n",
    "        return goes_ts, observatory\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Error inesperado al descargar datos GOES de {start_time}: {e}\"\n",
    "        print(msg)\n",
    "        if block_dir is not None:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        return None, None  #  siempre devuelve una tupla\n",
    "    \n",
    "\n",
    "## 1.2. Truncar datos\n",
    "def Truncate_Data(goes_ts, flare_start_time, flare_end_time):\n",
    "    \n",
    "    \"\"\"\n",
    "    Entrada / Input:\n",
    "        goes_ts (TimeSeries): Objeto TimeSeries con los datos GOES completos.\n",
    "        flare_start_time (str): Tiempo de inicio de la fulguraci√≥n (en formato compatible con SunPy).\n",
    "        flare_end_time (str): Tiempo de fin de la fulguraci√≥n (en formato compatible con SunPy).\n",
    "\n",
    "    Salida / Output:\n",
    "        TimeSeries: Objeto TimeSeries con los datos recortados al intervalo de la fulguraci√≥n.\n",
    "\n",
    "    Descripci√≥n / Description:\n",
    "        Esta funci√≥n recorta un conjunto de datos GOES a un intervalo de tiempo espec√≠fico\n",
    "        correspondiente al inicio y fin de una fulguraci√≥n solar. Si el intervalo no contiene datos,\n",
    "        se lanza una excepci√≥n.\n",
    "\n",
    "        This function trims a GOES TimeSeries dataset to a specific time interval\n",
    "        corresponding to the start and end of a solar flare. If the interval contains no data,\n",
    "        an exception is raised.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seleccionar el rango de inter√©s / Select the time range of interest\n",
    "    goes_flare = goes_ts.truncate(flare_start_time, flare_end_time)\n",
    "\n",
    "    # Verificar si hay datos disponibles  / Check if data is available\n",
    "    if len(goes_flare.to_dataframe()) == 0:\n",
    "        raise ValueError(\"El rango de datos seleccionado est√° vac√≠o. Revisa las fechas.\")\n",
    "        # The selected time range is empty. Please check the input times.\n",
    "\n",
    "    # Visualizar los datos truncados / Plot the trimmed data\n",
    "    goes_flare.peek()\n",
    "\n",
    "    return goes_flare\n",
    "\n",
    "## 2. Background (1)\n",
    "def running_difference(goes_ts, Dif_time=5, plot=False, block_dir=None, start_time=None):\n",
    "    \"\"\"\n",
    "    Calcula las diferencias de flujo de rayos X GOES a un intervalo definido (default 5 min).\n",
    "    \n",
    "    Par√°metros\n",
    "    ----------\n",
    "    goes_ts : XRSTimeSeries\n",
    "        Serie temporal original de GOES.\n",
    "    Dif_time : int, optional\n",
    "        Intervalo de diferencia en n√∫mero de pasos para restar flux (default=5).\n",
    "    plot : bool, optional\n",
    "        Si True, guarda gr√°ficas comparativas original vs corregido.\n",
    "    block_dir : str, optional\n",
    "        Carpeta base donde guardar gr√°ficas.\n",
    "    start_time : str, optional\n",
    "        Tiempo inicial usado para nombrar archivos.\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    goes_diff_ts : XRSTimeSeries\n",
    "        Serie temporal corregida con las diferencias.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.ticker import LogFormatterMathtext\n",
    "\n",
    "    # --- 1. Extraer datos ---\n",
    "    df = goes_ts.to_dataframe()\n",
    "    flux_xrsa = df[\"xrsa\"]\n",
    "    flux_xrsb = df[\"xrsb\"]\n",
    "    npts = len(df)\n",
    "\n",
    "    # --- 2. Calcular diferencias ---\n",
    "    diffa = np.array(flux_xrsa[Dif_time:]) - np.array(flux_xrsa[:npts - Dif_time])\n",
    "    diffb = np.array(flux_xrsb[Dif_time:]) - np.array(flux_xrsb[:npts - Dif_time])\n",
    "\n",
    "    # --- 3. Llenar arreglos completos ---\n",
    "    diffa_full = np.zeros(npts)\n",
    "    diffb_full = np.zeros(npts)\n",
    "    diffa_full[Dif_time:] = diffa\n",
    "    diffb_full[Dif_time:] = diffb\n",
    "\n",
    "    # --- 4. Crear DataFrame corregido ---\n",
    "    df_diff = pd.DataFrame({'xrsa': diffa_full, 'xrsb': diffb_full}, index=df.index)\n",
    "\n",
    "    # --- 5. Crear TimeSeries corregida ---\n",
    "    units = {'xrsa': u.W / u.m**2, 'xrsb': u.W / u.m**2}\n",
    "    goes_diff_ts = XRSTimeSeries(df_diff, units=units, meta=goes_ts.meta)\n",
    "\n",
    "    # --- 6. Funci√≥n auxiliar para graficar ---\n",
    "    def save_plot(df_orig, df_corr, output_file, title=\"\", logscale=False, positive_only=False):\n",
    "        df_o, df_c = df_orig.copy(), df_corr.copy()\n",
    "        if positive_only:\n",
    "            df_o = df_o.clip(lower=1e-9)\n",
    "            df_c = df_c.clip(lower=1e-9)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        ax.plot(df_o.index, df_o['xrsa'], label='XRSA (original)', color='blue')\n",
    "        ax.plot(df_c.index, df_c['xrsa'], label='XRSA (corrected)', color='blue', linestyle='--')\n",
    "        ax.plot(df_o.index, df_o['xrsb'], label='XRSB (original)', color='red')\n",
    "        ax.plot(df_c.index, df_c['xrsb'], label='XRSB (corrected)', color='red', linestyle='--')\n",
    "\n",
    "        date_only = df_orig.index[0].strftime(\"%Y-%m-%d\")\n",
    "        ax.set_xlabel(f\"Time (UTC) ‚Äî {date_only}\")\n",
    "        ax.set_ylabel(\"Flux [W/m¬≤]\")\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', ls='--', alpha=0.6)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "        if logscale:\n",
    "            ax.set_yscale('log', base=10)\n",
    "            ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Plot saved at: {output_file}\")\n",
    "\n",
    "    # --- 7. Guardar gr√°ficas si plot=True ---\n",
    "    if plot and block_dir is not None and start_time is not None:\n",
    "        graph_dir = os.path.join(block_dir, \"data_graphs\")\n",
    "        os.makedirs(graph_dir, exist_ok=True)\n",
    "\n",
    "        safe_time = start_time.replace(':','-').replace(' ','_')\n",
    "        df_corr = df_diff\n",
    "\n",
    "        # Linear\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_linear_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison (Œît={Dif_time} steps)\")\n",
    "\n",
    "        # Logarithmic\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_log_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison (Œît={Dif_time} steps) [Log Y]\",\n",
    "                  logscale=True)\n",
    "\n",
    "        # Positive only\n",
    "        save_plot(df, df_corr, os.path.join(graph_dir, f\"GOES_diff_positive_{safe_time}.png\"),\n",
    "                  title=f\"GOES Data Comparison [Positive only, Log Y]\",\n",
    "                  logscale=True,\n",
    "                  positive_only=True)\n",
    "\n",
    "    return goes_diff_ts\n",
    "\n",
    "\n",
    "#####################################\n",
    "# 5. Temperatura y Emission Measure #\n",
    "#####################################\n",
    "\n",
    "## 5.1. Calcular la Temperatura y Medida de emisi√≥n con los datos corregidos (C1)\n",
    "#Calculate_Tem_EM(goes_flare_corrected, abundance='photospheric')\n",
    "#Calculate_Tem_EM(goes_flare_corrected, abundance='coronal')\n",
    "def Calculate_Tem_EM(goes_flare_corrected, abundance='coronal'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Entrada / Input:\n",
    "        goes_flare_corrected: Objeto TimeSeries corregido que contiene los datos del flare solar en los\n",
    "        canales XRSA y XRSB. Debe ser compatible con la funci√≥n `calculate_temperature_em` de SunPy.\n",
    "        abundance (str): Tipo de abundancia elemental a usar en el c√°lculo (por defecto: 'coronal').\n",
    "                         Otras opciones posibles incluyen 'photospheric', dependiendo del modelo de SunPy.\n",
    "\n",
    "    Salida / Output:\n",
    "        temp_em: Objeto que contiene la temperatura (T) y medida de emisi√≥n (EM) derivadas a partir de los\n",
    "        datos GOES corregidos.\n",
    "\n",
    "    Descripci√≥n / Description:\n",
    "        Esta funci√≥n utiliza los datos corregidos del sat√©lite GOES para calcular la temperatura del plasma\n",
    "        y la medida de emisi√≥n (EM) durante un evento de fulguraci√≥n solar. Permite especificar el modelo\n",
    "        de abundancia elemental a utilizar en el c√°lculo.\n",
    "\n",
    "        This function uses corrected GOES data to compute the plasma temperature and emission measure (EM)\n",
    "        during a solar flare. It allows specifying the elemental abundance model to be used in the calculation.\n",
    "\n",
    "    Notas / Notes:\n",
    "        - Usa la funci√≥n `calculate_temperature_em` de SunPy.\n",
    "        - El par√°metro `abundance` controla el modelo de abundancias (por ejemplo, 'coronal' o 'photospheric').\n",
    "        - Se desactiva temporalmente la verificaci√≥n del hash de calibraci√≥n del instrumento.\n",
    "        - Los datos deben estar previamente corregidos y limpios.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Ahora vamos a calcular la T y EM con el modelo de abundancias:{abundance}')\n",
    "    #  Saltar la verificaci√≥n del hash temporalmente\n",
    "    with manager.skip_hash_check():\n",
    "        #temp_em = calculate_temperature_em(goes_flare_corrected, abundance='coronal')\n",
    "        temp_em = calculate_temperature_em(goes_flare_corrected, abundance)\n",
    "    \n",
    "    print(f'se calcul√≥ T y EM con el modelo de abundancias:{abundance}')\n",
    "    #print(temp_em)\n",
    "    return temp_em\n",
    "\n",
    "\n",
    "# mostrar un rango de datos de un TimeSeries\n",
    "def show_range_ts(ts, start_idx, end_idx, height=300):\n",
    "    df = ts.to_dataframe()\n",
    "    subdf = df.iloc[start_idx:end_idx]\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"height:{height}px; overflow:auto; border:1px solid #ccc; padding:10px\">\n",
    "        {subdf.to_html()}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "# Uso:\n",
    "#show_range_ts(goes_ts01, 0, 10)\n",
    "\n",
    "#####################################\n",
    "#     6. Calcula tiempos de FAI     #\n",
    "#####################################\n",
    "\n",
    "def calcular_fai_times(df, \n",
    "                       T_min=7, T_max=14, \n",
    "                       EM_threshold=0.005, \n",
    "                       col_T=\"T_cor\", \n",
    "                       col_EM=\"EM_cor_norm\"):\n",
    "    \"\"\"\n",
    "    Calcula los tiempos en los que se cumplen los criterios del √≠ndice FAI.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copiar para no modificar el original\n",
    "    df_fai = df.copy()\n",
    "\n",
    "    # Condiciones del criterio FAI\n",
    "    fai_condition = (\n",
    "        (df_fai[col_T] >= T_min) & \n",
    "        (df_fai[col_T] <= T_max) & \n",
    "        (df_fai[col_EM] > EM_threshold)\n",
    "    )\n",
    "\n",
    "    # Selecci√≥n\n",
    "    df_fai_selected = df_fai[fai_condition]\n",
    "    fai_times = df_fai_selected.index\n",
    "\n",
    "    # Mostrar los resultados como en tu c√≥digo original\n",
    "    print(f\"Se encontraron {len(df_fai_selected)} puntos que cumplen el criterio FAI.\\n\")\n",
    "    print(df_fai_selected[[col_T, col_EM]].head())\n",
    "\n",
    "    return fai_times, df_fai_selected\n",
    "\n",
    "########################\n",
    "# 7. Descargar Flares #\n",
    "#######################\n",
    "\n",
    "# Configurar logging b√°sico\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "## 7.1. Descarga de datos de flares\n",
    "def get_flares(start_time, end_time, output_dir=None):\n",
    "    \"\"\"\n",
    "    Busca fulguraciones solares reportadas por GOES en el intervalo dado.\n",
    "    y guarda un log si no se encuentran flares o si hay errores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_time : str\n",
    "        Tiempo inicial (YYYY-MM-DD o compatible con SunPy).\n",
    "    end_time : str\n",
    "        Tiempo final (YYYY-MM-DD o compatible con SunPy).\n",
    "    log_file : str o None, opcional\n",
    "        Ruta del archivo .log donde guardar mensajes si no hay flares o hay errores.\n",
    "        Si es None, no se escribe log en disco (solo en consola).\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame con columnas ['StartTime', 'EndTime', 'Class', 'Observatory', 'PeakTime'] \n",
    "        o None si no se encontraron flares.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Searching for GOES flares between {start_time} and {end_time}...\")\n",
    "        result = Fido.search(a.Time(start_time, end_time), a.hek.FL, a.hek.OBS.Observatory == \"GOES\")\n",
    "\n",
    "        if not result or len(result) == 0 or len(result[0]) == 0:\n",
    "            msg = f\"No solar flares found between {start_time} and {end_time}.\"\n",
    "            logging.info(msg)\n",
    "            if output_dir is not None:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                log_path = os.path.join(output_dir, \"no_flares.log\")\n",
    "                with open(log_path, \"a\") as f:\n",
    "                    f.write(msg + \"\\n\")\n",
    "            return None\n",
    "\n",
    "        # Filtrar columnas 1D\n",
    "        names = [name for name in result[0].colnames if len(result[0][name].shape) <= 1]\n",
    "        table = result[0][names].to_pandas()\n",
    "\n",
    "        # Seleccionar columnas de inter√©s\n",
    "        flare_data = table[[\n",
    "            \"event_starttime\",\n",
    "            \"event_endtime\",\n",
    "            \"fl_goescls\",\n",
    "            \"obs_observatory\",\n",
    "            \"event_peaktime\"\n",
    "        ]]\n",
    "\n",
    "        # Renombrar columnas\n",
    "        flare_data.columns = [\"StartTime\", \"EndTime\", \"Class\", \"Observatory\", \"PeakTime\"]\n",
    "\n",
    "        logging.info(f\"Found {len(flare_data)} GOES solar flares between {start_time} and {end_time}.\")\n",
    "        return flare_data\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"‚ùå Error retrieving flares between {start_time} and {end_time}: {e}\"\n",
    "        logging.error(msg)\n",
    "\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            log_path = os.path.join(output_dir, \"no_flares.log\")\n",
    "            with open(log_path, \"a\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def build_full_dataframe(goes_ts, goes_corrected, temp_em_cor, temp_em_phot,\n",
    "                         clip_negative=True, normalize_em=False):\n",
    "    \"\"\"\n",
    "    Combina datos originales, corregidos y par√°metros de temperatura/EM\n",
    "    en un solo DataFrame.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    goes_ts : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal GOES remuestreada (contiene 'xrsa', 'xrsb').\n",
    "    goes_corrected : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con GOES corregido (xrsa, xrsb corregidos).\n",
    "    temp_em_cor : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con temperatura y EM coronal.\n",
    "    temp_em_phot : sunpy.timeseries.TimeSeries\n",
    "        Serie temporal con temperatura y EM fotosf√©rica.\n",
    "    clip_negative : bool, opcional\n",
    "        Si True, reemplaza valores negativos con NaN (en lugar de 0).\n",
    "    normalize_em : bool, opcional\n",
    "        Si True, normaliza EM a unidades de 1e49 cm^-3.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame combinado con todas las columnas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Originales\n",
    "    df_original = goes_ts.to_dataframe()[['xrsa', 'xrsb']]\n",
    "\n",
    "    # Corregidos\n",
    "    df_corr = goes_corrected.to_dataframe().rename(\n",
    "        columns={'xrsa': 'xrsa_corr', 'xrsb': 'xrsb_corr'}\n",
    "    )\n",
    "\n",
    "    # Coronal\n",
    "    df_cor = temp_em_cor.to_dataframe()[['temperature', 'emission_measure']].rename(\n",
    "        columns={'temperature': 'T_cor', 'emission_measure': 'EM_cor'}\n",
    "    )\n",
    "\n",
    "    # Fotosf√©rica\n",
    "    df_phot = temp_em_phot.to_dataframe()[['temperature', 'emission_measure']].rename(\n",
    "        columns={'temperature': 'T_phot', 'emission_measure': 'EM_phot'}\n",
    "    )\n",
    "\n",
    "    # Combinar todo\n",
    "    df_full = pd.concat([df_original, df_corr, df_cor, df_phot], axis=1)\n",
    "\n",
    "    # Opcional: reemplazar valores negativos\n",
    "    if clip_negative:\n",
    "        df_full = df_full.mask(df_full < 0, np.nan)   # ahora quedan NaN y no 0\n",
    "\n",
    "    # Opcional: normalizar EM\n",
    "    if normalize_em:\n",
    "        df_full['EM_cor_norm'] = df_full['EM_cor'] / 1e49\n",
    "        df_full['EM_phot_norm'] = df_full['EM_phot'] / 1e49\n",
    "\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# agrupa por clases de flares en 2 grupos por clase\n",
    "def assign_flare_group(flare_class):\n",
    "    \"\"\"\n",
    "    Asigna un grupo a un flare seg√∫n su clase y subnivel.\n",
    "    \n",
    "    Ejemplos:\n",
    "    - C3.0 ‚Üí C1-4\n",
    "    - C7.5 ‚Üí C5-9\n",
    "    - X2.0 ‚Üí X1-4\n",
    "    - X7.0 ‚Üí X5+\n",
    "    \"\"\"\n",
    "    group_ranges = {\n",
    "        \"A\": [(1, 4), (5, 9)],\n",
    "        \"B\": [(1, 4), (5, 9)],\n",
    "        \"C\": [(1, 4), (5, 9)],\n",
    "        \"M\": [(1, 4), (5, 9)],\n",
    "        \"X\": [(1, 4), (5, 1000)]  # 1000 act√∫a como \"infinito\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        letter = flare_class[0]  # A, B, C, M, X\n",
    "        number = float(flare_class[1:])  # acepta decimales, ej: \"2.7\" ‚Üí 2.7\n",
    "    except:\n",
    "        return flare_class  # si algo raro viene en el CSV\n",
    "\n",
    "    if letter not in group_ranges:\n",
    "        return flare_class\n",
    "\n",
    "    for (low, high) in group_ranges[letter]:\n",
    "        if low <= number <= high:\n",
    "            if high >= 1000:  # caso abierto (ej: X5+)\n",
    "                return f\"{letter}{low}+\"\n",
    "            return f\"{letter}{low}-{high}\"\n",
    "    \n",
    "    return flare_class\n",
    "\n",
    "\n",
    "#######################\n",
    "### cuenta los d√≠as ###\n",
    "#######################\n",
    "\n",
    "def count_days(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Counts the number of days between two dates.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of days between start_date and end_date\n",
    "    \"\"\"\n",
    "    # Convert the strings to datetime objects\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = end - start\n",
    "    \n",
    "    return difference.days\n",
    "\n",
    "# Example usage\n",
    "#days = count_days(\"1980-01-05\", \"2025-08-20\")\n",
    "#print(f\"Days between the dates: {days}\")\n",
    "\n",
    "# Dataframe con todas las fechas\n",
    "def all_dates_dataframe(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con todas las fechas entre start_date y end_date (inclusive).\n",
    "\n",
    "    Par√°metros:\n",
    "    - start_date (str): Fecha inicial en formato 'YYYY-MM-DD'\n",
    "    - end_date (str): Fecha final en formato 'YYYY-MM-DD'\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame con columnas ['start_time', 'end_time']\n",
    "    \"\"\"\n",
    "    # Convertir a objetos datetime\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Generar todas las fechas d√≠a a d√≠a\n",
    "    all_days = [start + timedelta(days=i) for i in range((end - start).days + 1)]\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    data = {\n",
    "        \"start_time\": [d.strftime(\"%Y-%m-%d 00:00:00\") for d in all_days],\n",
    "        \"end_time\":   [d.strftime(\"%Y-%m-%d 23:59:00\") for d in all_days]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#############################################\n",
    "# elige los d√≠as cada cierto paso seg√∫n     #\n",
    "# el n√∫mero de d√≠as que quiera analizar (n) #\n",
    "#############################################\n",
    "\n",
    "def select_dates(start_date, end_date, n=10):\n",
    "    \"\"\"\n",
    "    Selects n dates evenly spaced between start_date and end_date.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format\n",
    "    - n (int): Number of dates to select (default 10)\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of dates in 'YYYY-MM-DD' format\n",
    "    \"\"\"\n",
    "    # Convert strings to datetime objects\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate total number of days\n",
    "    total_days = (end - start).days\n",
    "    \n",
    "    # Calculate integer step\n",
    "    #step = total_days // (n - 1)  # n-1 intervals for n dates\n",
    "    \n",
    "    if n < 2:\n",
    "        return [start_date]\n",
    "\n",
    "    # step aproximado al entero m√°s cercano\n",
    "    step = max(1, int(round(total_days / (n - 1))))\n",
    "    print(step)\n",
    "    # Generate the dates\n",
    "    dates = [start + timedelta(days=i*step) for i in range(n)]\n",
    "    print(len(dates))\n",
    "    # Convert to strings\n",
    "    return [d.strftime(\"%Y-%m-%d\") for d in dates]\n",
    "\n",
    "\n",
    "# guarda los datos seleccionados segun el paso en un df\n",
    "def dates_to_dataframe(dates_list):\n",
    "    \"\"\"\n",
    "    Converts a list of dates into a DataFrame with start_time and end_time.\n",
    "\n",
    "    Parameters:\n",
    "    - dates_list (list of str): List of dates in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with columns ['start_time', 'end_time']\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"start_time\": [f\"{date} 00:00:00\" for date in dates_list],\n",
    "        \"end_time\":   [f\"{date} 23:59:00\" for date in dates_list]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Divide un dateframen en lotes:\n",
    "def chunk_dataframe(df, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en bloques (chunks) de tama√±o chunk_size.\n",
    "    Devuelve una lista de DataFrames.\n",
    "    \"\"\"\n",
    "    return [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# unir todos los bloques al final\n",
    "def combine_blocks(output_dir, pattern, final_name, time_column=None):\n",
    "    \"\"\"\n",
    "    Une todos los CSV de bloques en un √∫nico archivo final.\n",
    "    Si los CSV tienen la primera columna como fecha sin nombre, se usa para ordenar.\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(output_dir, \"**\", f\"*{pattern}*.csv\"), recursive=True))\n",
    "    df_list = []\n",
    "\n",
    "    for f in files:\n",
    "        # Leer CSV; si la primera columna no tiene nombre, le ponemos \"time\"\n",
    "        df = pd.read_csv(f)\n",
    "        if df.columns[0] == \"\":\n",
    "            df = pd.read_csv(f, names=[\"time\"] + list(df.columns[1:]), header=0)\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Ordenar seg√∫n columna de tiempo\n",
    "        if time_column is None:\n",
    "            # usar la primera columna como tiempo\n",
    "            combined.iloc[:, 0] = pd.to_datetime(combined.iloc[:, 0])\n",
    "            combined = combined.sort_values(by=combined.columns[0]).reset_index(drop=True)\n",
    "            print(f\"‚úÖ Ordenado por la primera columna (usada como tiempo)\")\n",
    "        elif time_column in combined.columns:\n",
    "            combined[time_column] = pd.to_datetime(combined[time_column])\n",
    "            combined = combined.sort_values(by=time_column).reset_index(drop=True)\n",
    "            print(f\"‚úÖ Ordenado por columna '{time_column}'\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Columna '{time_column}' no encontrada, no se orden√≥\")\n",
    "\n",
    "        combined.to_csv(os.path.join(output_dir, final_name), index=False)\n",
    "        print(f\"{final_name} creado con {len(combined)} filas (desde {len(files)} archivos).\")\n",
    "    else:\n",
    "        print(f\"No se encontraron archivos con patr√≥n {pattern}\")\n",
    "\n",
    "# Example usage\n",
    "#dates_list = select_dates(\"1980-01-05\", \"2025-08-20\", n=10)\n",
    "#df_intervals = dates_to_dataframe(dates_list)\n",
    "\n",
    "#print(df_intervals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf67b99",
   "metadata": {},
   "source": [
    "## Funci√≥n completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bd5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_goes_flare_data(start_time, end_time, \n",
    "                      resolution=\"avg1m\",  #resoluci√≥n de descarga de los datos GOES (1min)\n",
    "                      Dif_time=5,            #Difference time Œît (for background)\n",
    "                      plot_diff=True,\n",
    "                      output_dir=None):\n",
    "    \"\"\"\n",
    "    Pipeline completo para an√°lisis de datos GOES y c√°lculo de FAI y anticipaci√≥n.\n",
    "\n",
    "    Par√°metros:\n",
    "    -----------\n",
    "    start_time : str\n",
    "        Tiempo inicial (ej: \"2017-09-06 12:00:00\")\n",
    "    end_time : str\n",
    "        Tiempo final (ej: \"2017-09-06 12:15:00\")\n",
    "    resolution : str, opcional\n",
    "        Resoluci√≥n de datos GOES, default \"avg1m\". Opciones: [\"flx1s\", \"avg1m\"]\n",
    "    Dif_time : int, opcional\n",
    "        Ventana en pasos para calcular diferencias (default 5).\n",
    "    plot_diff : bool, opcional\n",
    "        Si True, grafica las diferencias calculadas.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict con:\n",
    "        - df_full : DataFrame con todos los c√°lculos\n",
    "        - df_flare_data : DataFrame de flares GOES en el intervalo\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Descargar datos GOES\n",
    "    print(\"1. Descargar datos GOES\")\n",
    "    data = Download_Data(start_time, end_time, resolution, log_file=\"errores_goes.log\", output_dir=output_dir)\n",
    "    # para d√≠as sin datos GOES\n",
    "    if data is None:\n",
    "        print(f\"No hay datos GOES para {start_time} - {end_time}. D√≠a saltado.\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Se encontraron datos  GOES para {start_time} - {end_time}. D√≠a saltado.\\n\")\n",
    "        goes_ts, observatory = data\n",
    "        # Convertir TimeSeries a DataFrame para inspecci√≥n\n",
    "        df_goes = goes_ts.to_dataframe()\n",
    "\n",
    "        # N√∫mero de registros\n",
    "        print(f\"N√∫mero de registros: {len(df_goes)}\")\n",
    "\n",
    "        # Columnas disponibles\n",
    "        print(f\"Columnas disponibles: {list(df_goes.columns)}\")\n",
    "        \n",
    "    print(\"2. Restar Background\")\n",
    "    # 2. Calcular diferencias\n",
    "    goes_ts_corrected_diff = running_difference(goes_ts, Dif_time=Dif_time, plot=plot_diff, block_dir=output_dir, start_time=start_time)\n",
    "                            \n",
    "\n",
    "    # 3. Calcular T y EM (coronal y fotosf√©rico)\n",
    "    print(\"3. USAR FUNCI√ìN SUNPY calculate_t_em\")\n",
    "    temp_em_cor = Calculate_Tem_EM(goes_ts_corrected_diff, abundance='coronal')\n",
    "    temp_em_phot = Calculate_Tem_EM(goes_ts_corrected_diff, abundance='photospheric')\n",
    "\n",
    "    print(\"4. CONSTRUIR df_full\")\n",
    "    # 4. Construir dataframe completo\n",
    "    df_full = build_full_dataframe(goes_ts, goes_ts_corrected_diff, temp_em_cor, temp_em_phot,\n",
    "                         clip_negative=True, normalize_em=True)\n",
    "\n",
    "    print(f\"5. a√±adiendo observatorio: GOES: {observatory}\")\n",
    "    # 5. A√±adir columna del observatorio\n",
    "    df_full[\"observatory\"] = observatory\n",
    "    # Reemplazar NaN por \"Unknown\"\n",
    "    df_full[\"observatory\"] = df_full[\"observatory\"].fillna(\"Unknown\")\n",
    "    # Mover \"observatory\" al inicio\n",
    "    cols = [\"observatory\"] + [col for col in df_full.columns if col != \"observatory\"]\n",
    "    df_full = df_full[cols]\n",
    "    #print(df_full)\n",
    "    print(f\"se a√±adi√≥ observatorio: GOES: {observatory}\")\n",
    "\n",
    "    #print(f\"6. Normalizando EM\")\n",
    "    # 6. Normalizar EM\n",
    "    #df_full['EM_cor_norm'] = df_full['EM_cor'] / 1e49\n",
    "    #df_full['EM_phot_norm'] = df_full['EM_phot'] / 1e49\n",
    "    \n",
    "    print(f\"6. Descargando flares: {start_time} - {end_time}\")\n",
    "    # 6. Descargar flares\n",
    "    flare_data = get_flares(start_time, end_time, output_dir=output_dir)\n",
    "    \n",
    "    #verifica si encontr√≥ flares para el d√≠a\n",
    "    if flare_data is None:\n",
    "        print(f\"No se encontraron flares para el intervalo {start_time} - {end_time}. Saltando...\")\n",
    "        # Crear DataFrames vac√≠os para todos los resultados que dependen de flares\n",
    "        df_flare_data = pd.DataFrame()\n",
    "    \n",
    "        \n",
    "        # Retornar diccionario con flares vac√≠os\n",
    "        return {\n",
    "            \"df_full\": df_full,\n",
    "            \"df_flare_data\": df_flare_data,\n",
    "        }\n",
    "    \n",
    "\n",
    "    # Si flare_data no es None contin√∫a\n",
    "    print(f\"Se descargaron flares para el intervalo {start_time} - {end_time}.\")\n",
    "    df_flare_data = flare_data[\n",
    "        flare_data['Class'].notna() & \n",
    "        (flare_data['Class'].str.strip() != \"\") &\n",
    "        (flare_data['Observatory'] == \"GOES\")\n",
    "    ]\n",
    "    print(f\"7. Se filtraron solo flares de GOES.\")\n",
    "      \n",
    "\n",
    "    # Retornar todo como dict\n",
    "    return {\n",
    "        \"df_full\": df_full,\n",
    "        \"df_flare_data\": df_flare_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fcdb6",
   "metadata": {},
   "source": [
    "## D√≠as para cada bloque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days between the dates: 283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7753424657534247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size=365 # d√≠as para cada bloque\n",
    "\n",
    "# contamos cuantos d√≠as hay desde la primera fecha 1980-01-05 hasta hoy\n",
    "year = 1980\n",
    "days = count_days(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "print(f\"Days between the dates: {days}\")\n",
    "\n",
    "days_per_block = days/chunk_size\n",
    "days_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e923b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              start_time             end_time\n",
      "0    1980-01-01 00:00:00  1980-01-01 23:59:00\n",
      "1    1980-01-02 00:00:00  1980-01-02 23:59:00\n",
      "2    1980-01-03 00:00:00  1980-01-03 23:59:00\n",
      "3    1980-01-04 00:00:00  1980-01-04 23:59:00\n",
      "4    1980-01-05 00:00:00  1980-01-05 23:59:00\n",
      "..                   ...                  ...\n",
      "279  1980-10-06 00:00:00  1980-10-06 23:59:00\n",
      "280  1980-10-07 00:00:00  1980-10-07 23:59:00\n",
      "281  1980-10-08 00:00:00  1980-10-08 23:59:00\n",
      "282  1980-10-09 00:00:00  1980-10-09 23:59:00\n",
      "283  1980-10-10 00:00:00  1980-10-10 23:59:00\n",
      "\n",
      "[284 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_days = all_dates_dataframe(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "print(df_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbba1b2",
   "metadata": {},
   "source": [
    "## New folder for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbffd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener fecha actual en formato YYYY-MM-DD\n",
    "fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#fecha_actual =\"2025-10-07\"\n",
    "# Crear nombre din√°mico de la carpeta\n",
    "output_dir = f\"{fecha_actual}_Analysis_for_{year}\"\n",
    "#output_dir = f\"2025-10-07_Analysis_for_1000_days\"\n",
    "# Crear carpeta (si no existe)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee842",
   "metadata": {},
   "source": [
    "## Procesar por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34a5a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funci√≥n para procesar y guardar datos por mes\n",
    "def process_goes_by_month(df_days, output_dir):\n",
    "    \"\"\"\n",
    "    Procesa los datos GOES agrupando los d√≠as por mes.\n",
    "\n",
    "    Crea una carpeta por mes (YYYY_MM) dentro de output_dir,\n",
    "    y guarda los CSV 'df_full' y 'df_flare_data' para cada mes.\n",
    "\n",
    "    Par√°metros:\n",
    "    - df_days: DataFrame con columnas ['start_time', 'end_time']\n",
    "    - output_dir: ruta base donde se guardar√°n los resultados\n",
    "    \"\"\"\n",
    "    # Asegurar que las columnas de fecha son tipo datetime\n",
    "    df_days[\"start_time\"] = pd.to_datetime(df_days[\"start_time\"])\n",
    "    df_days[\"end_time\"]   = pd.to_datetime(df_days[\"end_time\"])\n",
    "\n",
    "    # Crear columna auxiliar con el mes en formato YYYY_MM\n",
    "    df_days[\"month\"] = df_days[\"start_time\"].dt.strftime(\"%Y_%m\")\n",
    "\n",
    "    # Agrupar por mes\n",
    "    for month, df_month in df_days.groupby(\"month\"):\n",
    "        print(f\"\\nüü¶ Procesando mes {month} ({len(df_month)} d√≠as)\")\n",
    "\n",
    "        # Crear carpeta para este mes\n",
    "        month_dir = os.path.join(output_dir, f\"Month_{month}\")\n",
    "        log_file = os.path.join(month_dir, \"errores_goes.log\")\n",
    "        os.makedirs(month_dir, exist_ok=True)\n",
    "\n",
    "        # Archivos esperados\n",
    "        file_full_month  = os.path.join(month_dir, f\"df_full_{month}.csv\")\n",
    "        file_flare_month = os.path.join(month_dir, f\"df_flare_data_{month}.csv\")\n",
    "\n",
    "        # Saltar si ya existen ambos\n",
    "        if os.path.exists(file_full_month) and os.path.exists(file_flare_month):\n",
    "            print(f\"‚ö†Ô∏è Mes {month} ya procesado, saltando...\")\n",
    "            continue\n",
    "\n",
    "        list_df_full = []\n",
    "        list_df_flare_data = []\n",
    "\n",
    "        # Procesar cada d√≠a dentro del mes\n",
    "        for idx, row in df_month.iterrows():\n",
    "            start_time = row[\"start_time\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            end_time   = row[\"end_time\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            date_str   = row[\"start_time\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            print(f\"\\nüöÄ Procesando {date_str} ({start_time} ‚Üí {end_time})...\")\n",
    "\n",
    "            try:\n",
    "                results = download_goes_flare_data(   #funci√≥n completa\n",
    "                                    start_time, end_time,\n",
    "                                    resolution=\"avg1m\",\n",
    "                                    Dif_time=5,\n",
    "                                    plot_diff=True,\n",
    "                                    output_dir=month_dir\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error en {date_str}: {e}\")\n",
    "                results = None\n",
    "\n",
    "            if results is None:\n",
    "                print(f\"‚ö†Ô∏è Sin resultados para {date_str}\")\n",
    "                continue\n",
    "\n",
    "            df_full = results[\"df_full\"]\n",
    "            df_flare_data = results[\"df_flare_data\"]\n",
    "\n",
    "            if not df_full.empty:\n",
    "                list_df_full.append(df_full)\n",
    "                print(f\"‚úÖ df_full agregado ({date_str})\")\n",
    "            if not df_flare_data.empty:\n",
    "                list_df_flare_data.append(df_flare_data)\n",
    "                print(f\"‚úÖ df_flare_data agregado ({date_str})\")\n",
    "\n",
    "        # Guardar resultados del mes\n",
    "        if list_df_full:\n",
    "            pd.concat(list_df_full).to_csv(file_full_month, index=True)\n",
    "            print(f\"üíæ df_full_{month}.csv guardado\")\n",
    "        else:\n",
    "            pd.DataFrame().to_csv(file_full_month, index=False)\n",
    "            print(f\"‚ÑπÔ∏è df_full_{month}.csv vac√≠o\")\n",
    "\n",
    "        if list_df_flare_data:\n",
    "            pd.concat(list_df_flare_data).to_csv(file_flare_month, index=True)\n",
    "            print(f\"üíæ df_flare_data_{month}.csv guardado\")\n",
    "        else:\n",
    "            pd.DataFrame().to_csv(file_flare_month, index=False)\n",
    "            print(f\"‚ÑπÔ∏è df_flare_data_{month}.csv vac√≠o\")\n",
    "\n",
    "        print(f\"üèÅ Mes {month} terminado ‚Üí {month_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598c5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¶ Procesando mes 2008_02 (2 d√≠as)\n",
      "\n",
      "üöÄ Procesando 2008-02-03 (2008-02-03 00:00:00 ‚Üí 2008-02-03 23:59:00)...\n",
      "1. Descargar datos GOES\n",
      "Buscando datos de: 2008-02-03 00:00:00\n",
      "Descargando datos de 2008-02-03 00:00:00...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.69file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution = 1 minute\n",
      "Gr√°fica guardada en 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_2008-02-03_00-00-00.png\n",
      "Observatorio encontrado: GOES-11\n",
      "Se encontraron datos  GOES para 2008-02-03 00:00:00 - 2008-02-03 23:59:00. D√≠a saltado.\n",
      "\n",
      "N√∫mero de registros: 1440\n",
      "Columnas disponibles: ['xrsa', 'xrsb', 'xrsa_quality', 'xrsb_quality']\n",
      "2. Restar Background\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_linear_2008-02-03_00-00-00.png\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_log_2008-02-03_00-00-00.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1962144/3253930153.py:225: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_o = df_o.clip(lower=1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_positive_2008-02-03_00-00-00.png\n",
      "3. USAR FUNCI√ìN SUNPY calculate_t_em\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:coronal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32file/s]\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: divide by zero encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: invalid value encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calcul√≥ T y EM con el modelo de abundancias:coronal\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:photospheric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded:   0%|          | 0/1 [00:00<?, ?file/s]Exception ignored in: <function BaseEventLoop.__del__ at 0x7f3e3a99b420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/base_events.py\", line 766, in __del__\n",
      "    self.close()\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 73, in close\n",
      "    self.remove_signal_handler(sig)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 162, in remove_signal_handler\n",
      "    signal.signal(sig, handler)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "ValueError: signal only works in main thread of the main interpreter\n",
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.28file/s]\n",
      "2025-10-16 17:43:37 - root - INFO: Searching for GOES flares between 2008-02-03 00:00:00 and 2008-02-03 23:59:00...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calcul√≥ T y EM con el modelo de abundancias:photospheric\n",
      "4. CONSTRUIR df_full\n",
      "5. a√±adiendo observatorio: GOES: GOES-11\n",
      "se a√±adi√≥ observatorio: GOES: GOES-11\n",
      "6. Descargando flares: 2008-02-03 00:00:00 - 2008-02-03 23:59:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 17:43:38 - root - INFO: No solar flares found between 2008-02-03 00:00:00 and 2008-02-03 23:59:00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron flares para el intervalo 2008-02-03 00:00:00 - 2008-02-03 23:59:00. Saltando...\n",
      "‚úÖ df_full agregado (2008-02-03)\n",
      "\n",
      "üöÄ Procesando 2008-02-04 (2008-02-04 00:00:00 ‚Üí 2008-02-04 23:59:00)...\n",
      "1. Descargar datos GOES\n",
      "Buscando datos de: 2008-02-04 00:00:00\n",
      "Descargando datos de 2008-02-04 00:00:00...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.75file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution = 1 minute\n",
      "Gr√°fica guardada en 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_2008-02-04_00-00-00.png\n",
      "Observatorio encontrado: GOES-11\n",
      "Se encontraron datos  GOES para 2008-02-04 00:00:00 - 2008-02-04 23:59:00. D√≠a saltado.\n",
      "\n",
      "N√∫mero de registros: 1440\n",
      "Columnas disponibles: ['xrsa', 'xrsb', 'xrsa_quality', 'xrsb_quality']\n",
      "2. Restar Background\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_linear_2008-02-04_00-00-00.png\n",
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_log_2008-02-04_00-00-00.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1962144/3253930153.py:225: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_o = df_o.clip(lower=1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved at: 2025-10-16_Analysis_for_1980/Month_2008_02/data_graphs/GOES_diff_positive_2008-02-04_00-00-00.png\n",
      "3. USAR FUNCI√ìN SUNPY calculate_t_em\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:coronal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.24file/s]\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: divide by zero encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/site-packages/astropy/units/quantity.py:659: RuntimeWarning: invalid value encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calcul√≥ T y EM con el modelo de abundancias:coronal\n",
      "Ahora vamos a calcular la T y EM con el modelo de abundancias:photospheric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Downloaded:   0%|          | 0/1 [00:00<?, ?file/s]Exception ignored in: <function BaseEventLoop.__del__ at 0x7f3e3a99b420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/base_events.py\", line 766, in __del__\n",
      "    self.close()\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 73, in close\n",
      "    self.remove_signal_handler(sig)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/asyncio/unix_events.py\", line 162, in remove_signal_handler\n",
      "    signal.signal(sig, handler)\n",
      "  File \"/home/pjgonzalesp/miniforge3/envs/sunpy/lib/python3.13/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "ValueError: signal only works in main thread of the main interpreter\n",
      "Files Downloaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.31file/s]\n",
      "2025-10-16 17:43:51 - root - INFO: Searching for GOES flares between 2008-02-04 00:00:00 and 2008-02-04 23:59:00...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se calcul√≥ T y EM con el modelo de abundancias:photospheric\n",
      "4. CONSTRUIR df_full\n",
      "5. a√±adiendo observatorio: GOES: GOES-11\n",
      "se a√±adi√≥ observatorio: GOES: GOES-11\n",
      "6. Descargando flares: 2008-02-04 00:00:00 - 2008-02-04 23:59:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 17:43:52 - root - INFO: No solar flares found between 2008-02-04 00:00:00 and 2008-02-04 23:59:00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron flares para el intervalo 2008-02-04 00:00:00 - 2008-02-04 23:59:00. Saltando...\n",
      "‚úÖ df_full agregado (2008-02-04)\n",
      "üíæ df_full_2008_02.csv guardado\n",
      "‚ÑπÔ∏è df_flare_data_2008_02.csv vac√≠o\n",
      "üèÅ Mes 2008_02 terminado ‚Üí 2025-10-16_Analysis_for_1980/Month_2008_02\n"
     ]
    }
   ],
   "source": [
    "#df_days = all_dates_dataframe(\"2008-02-03\", \"2008-02-04\")\n",
    "process_goes_by_month(df_days, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "052ee379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ordenado por la primera columna (usada como tiempo)\n",
      "all_df_full_1980.csv creado con 362758 filas (desde 15 archivos).\n",
      "‚úÖ Ordenado por columna 'StartTime'\n",
      "all_df_flare_data_1980.csv creado con 1585 filas (desde 15 archivos).\n"
     ]
    }
   ],
   "source": [
    "# Unir cada salida\n",
    "combine_blocks(output_dir, \"df_full_block\", f\"all_df_full_{year}.csv\", time_column=None)\n",
    "combine_blocks(output_dir, \"df_flare_data_block\", f\"all_df_flare_data_{year}.csv\", time_column=\"StartTime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf30f6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
